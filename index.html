<!DOCTYPE html>
<html>

<head>
	<title> Image Completion | CS445 Final Project </title>
	<meta charset="utf-8">
	<style>
		* {
			clear: both;
			font-family: "Avenir Next", "Verdana", Sans-serif;
			font-weight: 400;
			line-height: 1.5em;
			color: #333;
		}
		body {
			margin: 0px;
			background-color: #fcfcfc;
		}
		b {
			font-weight: 600;
		}
		img {
			width: 50%;
			display: block;
			clear: none;
			float: left;
		}
		.result img:hover {
			/*opacity: 0.5;*/
			/* position: relative; */
			-webkit-transform: scale(1.7);
			-ms-transform: scale(1.7);
			transform: scale(1.7);
			box-shadow: 0 14px 28px rgba(0,0,0,0.5), 0 10px 10px rgba(0,0,0,0.22);

	    /*top: -25px;*/
	    /*left: -35px;*/
	    /* width: 75%; */
	    height: auto;
	    display: block;
	    z-index: 1;
		}
		.result img {
			width: 25%;
			transition: all .5s ease-in-out;
		}
		figure {
			clear: none;
			margin: auto;
			display: block;
			width: auto;
		}
		figcaption {
			text-align: center;
			font-size: .8em;
		}
		code {
			background-color: #eee;
			padding: 0 3px;
			border-radius: 3px;
			font-size: .9em;
		}
		.result {
			clear: none;
			float: left;
			display: block;
			margin: auto;
		}
		a {
			color: black;
		}
		iframe {
			display: block;
			margin: auto;
		}
		header {
			margin: 0px;
			padding: 0px;
		}
		/* The navigation bar */
		.navbar {
			background-color: #333;
			position: fixed;
			/* Set the navbar to fixed position */
			top: 0;
			/* Position the navbar at the top of the page */
			/* width: 100vw; */
			left: 0;
			right: 0;
			/* Full width */
			margin: 0px;
			padding: 0 20px;
		}
		/* Links inside the navbar */
		.navbar a {
			color: #fcfcfc;
			float: left;
			display: block;
			padding: 14px 8px;
			text-decoration: none;
			font-weight: 200;
		}
		.navbar a:hover {
			background-color: #999;
		}
		.navbar ul {
			padding: 0 30px;
			display: flex;
			align-items: stretch;
			/* Default */
			justify-content: space-between;
			width: 100%;
			margin: 0;
			padding: 0;
		}
		.navbar li {
			text-align: center;
			display: block;
			flex: 0 1 auto;
			/* Default */
			list-style-type: none;
			font-size: 1vw;
		}
		/* Main content */
		.main {
			display: block;
			max-width: 90%;
			width: 960px;
			margin: 10vh auto;
		}
	</style>
</head>

<body>

	<header>
		<div class="navbar">
			<ul>
				<li> <a href="#0">Image Completion</a> </li>
				<li> <a href="#1">OVERVIEW</a> </li>
				<li> <a href="#2">DATABASE SETUP</a> </li>
				<li> <a href="#3">SEMANTIC SCENE MATCHING</a> </li>
				<li> <a href="#4">LOCAL CONTEXT MATCHING</a> </li>
				<li> <a href="#5">RESULT</a> </li>
				<li> <a href="#6">REFERENCE</a> </li>
			</ul>
		</div>
	</header>

	<div class="main" id="0">

		<h1 align='center' style="font-size:40px;padding-top:20px"> Scene Completion With Large-scale Image Data Set </h1>
		<h1 align='center' style="font-size:30px"> CS 445 Final Project</h1>

		<p style="text-align:center"> Author: Xilun Jin (xjin12), Xin Wen (xinwen5), Xintong Wu (xwu68), Qile Zhi (qilezhi2) </p>

		<h2 id="1" style="padding-top:20px"> OVERVIEW </h2>

		<p> Our project is based on Hays et al. 2008 paper <a href="http://graphics.cs.cmu.edu/projects/scene-completion/" target="_blank">Scene Completion Using Millions of Photographs</a>. In this project, an image database with more than four hundred thousands photos will be set up, and the algorithm will be able to patch up the holes of an input image with images both structurally and semantically similar. </p>

		<h2 id="2" style="padding-top:20px"> DATABASE SETUP </h2>

		<p>To download large sets of images online, the Flicker API is used. We select several categories of images based on keywords such as <i>Chicago</i> and <i>lake</i>. Since Flicker API limit to return only 400 images per API request, we handle this issue by making multiple API calls according to the date of images were uploaded. During the downloading process, we notice that with such large amount of data, it need more than half a month to download all the images. To boost the download speed, we design and implement a multithread downloader in Python. Also, to avoid having invalid or useless images, we prune out all the photos smaller than 800*600px or have an edge larger than 10000 px. Finally, we managed to download around 450,000 images in parallel within 30 hours and used these images as the pool for image completion which will be discussed in following sections.
		</p>

		<p> After setting up a large image database. our algorithm will first select best semantically matching scenes and perform traditional template matching. </p>

		<h2 id="3" style="padding-top:20px"> SEMANTIC SCENE MATCHING </h2>

		<p>In order to implement a perfect image completion and avoid long searching time, we need to find a group of candidate images that match the scenes in input image semantically. To find the semantically similar images, we used the GIST descriptor to model the images. GIST descriptor is described in Oliva et al. 2001 paper <a href="http://cvcl.mit.edu/Papers/IJCV01-Oliva-Torralba.pdf" target="_blank">
			Modeling the shape of the scene: a holistic representation of the spatial envelope</a>. <code>GIST</code> is a low dimensional representation of an image, where some perceptual dimensions such as openness, roughness, expansion are considered. Images with scenes in the same categories will get close GIST descriptor.</p>

		<p>We use a Python library called <a href="https://github.com/tuttieee/lear-gist-python" target="_blank">Lear GIST python</a>, which implements the GIST descriptor in the paper.</p>
<!--
		<p>Here are some code from MATLAB version of GIST<br>
			<b>Computing the GIST descriptor</b><br>
			<code>
				% Load image<br/>
				img = imread('demo2.jpg');<br/>

				% GIST Parameters:<br/>
				clear param<br/>
				param.orientationsPerScale = [8 8 8 8]; % number of orientations per scale (from HF to LF)<br/>
				param.numberBlocks = 4;<br/>
				param.fc_prefilt = 4;<br/>

				% Computing gist:<br/>
				[gist, param] = LMgist(img, '', param);<br/>
			</code>
			<b>Image similarities</b><br>
			<code>
				% Load images<br/>
				img1 = imread('demo1.jpg');<br/>
				img2 = imread('demo2.jpg');<br/>

				% GIST Parameters:<br/>
				clear param<br/>
				param.imageSize = [256 256]; % it works also with non-square images (use the most common aspect ratio in your set)<br/>
				param.orientationsPerScale = [8 8 8 8]; % number of orientations per scale<br/>
				param.numberBlocks = 4;<br/>
				param.fc_prefilt = 4;<br/>

				% Computing gist:<br/>
				gist1 = LMgist(img1, '', param);<br/>
				gist2 = LMgist(img2, '', param);<br/>

				% Distance between the two images:<br/>
				D = sum((gist1-gist2).^2)<br/>

			</code>

		</p> -->

		<p>First, we calculate the GIST descriptor for every image using multiprocess algorithm in our image database and store all in .npy files for fast read and write.</p>

		<p>Second, we calculate a weighted GIST descriptor for the to-be-filled image with its mask for the hole. The GIST is weighted by the proportion of valid pixels(not in the hole) in every spatial bin. </p>

		<p>Finally, we calculate the L1 distance between the GIST for the input image and the GIST for images in the database. We also calculate the SSD (Sum of Square Distance) between the input image and images in the database. Then we assign weights to these two attributes and pick 20 images that have smallest values as the input images for the next step</p>

		<h2 id="4" style="padding-top:20px"> LOCAL CONTEXT MATCHING </h2>

		<p> We now have 20 semantically nearest scenes, and we can use our local template matching algorithm to find the best matchings. We further divide the steps into two: Seam Finding and Poisson Blending, inspired by previous <b>Project 2</b> and <b>Project 3</b>. </p>

		<p> We iterate the following process on each selected images: </p>

		<h3> Setup Input Image </h3>

		When the program start, we can manually select the area to "erase", or input the incomplete image and its mask.

		<h3> Cut Seam Finding </h3>

		<p> To find the minimum cost path around the irregular border, we first find the smallest rectangle containing the inrregular hole, then add 80 to each four borders to get a larger rectangle as well and minus 80 to each borders to get a smaller rectangles. We regard the difference between the large and small rectangles (the width of the border is <code>160</code> in our case) as local context, i.e., the overlapping region of the existing and sampled patch. </p>

		<b>Example of graph cut region</b><br>
		<img src="Data/border.png" alt="example"/>
		<p>Note the blue rectangle is the smallest rectangle containing the inrregular hole. Our graph cut is performed between yellow rectangle and orange rectangle.</p>

		<p> Take the top <code>160 * width</code> region as example. We use the idea of <code>buildErrpatch</code> in <b>Project 2</b>, but add some modifications to satisfy the need of irregular shape. First, we calculate the SSD of two images on <code>L*a*b</code> color space instead of RGB color space. Since the region we need to cut contains part of holes, we changed the overlap region of hole and cut to <code>Inf</code> so that it won't be marked as cut edge. Then, we use <code>cut</code> to find the minimum cost path from the left to the right side of the patch, which defines a binary mask for the whole local context region. </p>

		<p> We apply the above process to all four edges (top, bottom, left, right). Note that to find the cut for left and right region, we need to transpose the region first so that we can find the cutting path from top to the bottom side in the original region. We store the minimum cost for each time, and let the sum to be the total cut cost. Find the intersection of four masks. </p>

		<h3> Poisson Blending </h3>

		<p> After we get the mask, we use the standard <code>poissonBlend</code> by <b>Project 3</b> on the whole image region to compose the image. </p>

		<p> Note that different from <b>Project 3</b> which allows the user to select mask region, our mask is computed by the cut, so there might be isolated pixels or regions with wierd border. Since Poission Blending is based on neighboring regions, before outputing mask in the process above, we smooth the mask a little bit. </p>

		<p> We use the Poisson Blending formula to solve the blending constraints: <img src="./poissonblend_eq.png" alt="poisson blend equation"/></p><br>

		
		<h3> Calculate Costs and Ranks </h3>
		<p> To calculate costs, we add up three scores: GIST distance, graph cut cost and Poisson blending cost. After writing 20 output images, storing costs and ranking from smallest to largest, we select the top 5 matches.</p>

		<b> Sample Output </b> <br>
		No. 1 2005_10_13_31.jpg: <br>
		gistdist=0.072752<br>
		cutcost=8.921728 <br>
		blendcost=1.555726<br>

		No. 2 2005_10_3_33.jpg: gistdist=0.056394
		 cutcost=140.355418 blendcost=8.606997
		No. 3 2005_11_14_38.jpg: gistdist=0.039884
		 cutcost=35.207486 blendcost=3.811644
		No. 4 2005_11_38_49.jpg: gistdist=0.085163
		 cutcost=7.682632 blendcost=2.521401
		No. 5 2005_1_4_33.jpg: gistdist=0.092792
		 cutcost=356.132497 blendcost=6.827347

		<h2 id="5" style="padding-top:20px"> RESULT </h2>

		<h3 style="padding-top:30px"> Test Result 1 </h3>

		<figure>
			<img src="gist_res/test2/input.jpg" alt="test2"/>
			<img src="gist_res/test2/mask.jpg" alt="mask"/>
		</figure>

		<figure class="result">
			<img src="gist_res/test2/after13.jpg" alt="output"/>
			<img src="gist_res/test2/after9.jpg" alt="output"/>
			<img src="gist_res/test2/after6.jpg" alt="output"/>
			<img src="gist_res/test2/after14.jpg" alt="output"/>
		</figure>
		<figure class="result">
			<img src="gist_res/test2/2005_4_8_23.jpg" alt="output"/>
			<img src="gist_res/test2/2005_3_3_0.jpg" alt="output"/>
			<img src="gist_res/test2/2005_11_34_20.jpg" alt="output"/>
			<img src="gist_res/test2/2005_6_10_21.jpg" alt="output"/>
		</figure>

		<h3 style="padding-top:30px"> Test Result 2.1 </h3>

		<figure>
			<img src="gist_res/test1/input.jpg" alt="test1"/>
			<img src="gist_res/test1/mask.jpg" alt="mask"/>
		</figure>

		<figure class="result">
			<img src="gist_res/test1/after15.jpg" alt="output"/>
			<img src="gist_res/test1/after2.jpg" alt="output"/>
			<img src="gist_res/test1/after8.jpg" alt="output"/>
			<img src="gist_res/test1/after9.jpg" alt="output"/>
		</figure>
		<figure class="result">
			<img src="gist_res/test1/2005_7_12_2.jpg" alt="output"/>
			<img src="gist_res/test1/2005_10_3_33.jpg" alt="output"/>
			<img src="gist_res/test1/2005_3_37_6.jpg" alt="output"/>
			<img src="gist_res/test1/2005_4_8_11.jpg" alt="output"/>
		</figure>

		<h3 style="padding-top:30px"> Test Result 2.2 </h3>

		<figure>
			<img src="gist_res/test1 copy/input.jpg" alt="test1"/>
			<img src="gist_res/test1 copy/mask.jpg" alt="mask"/>
		</figure>

		<figure class="result">
			<img src="gist_res/test1 copy/after5.jpg" alt="output"/>
			<img src="gist_res/test1 copy/after2.jpg" alt="output"/>
			<img src="gist_res/test1 copy/after8.jpg" alt="output"/>
			<img src="gist_res/test1 copy/after17.jpg" alt="output"/>
		</figure>
		<figure class="result">
			<img src="gist_res/test1 copy/2005_1_4_33.jpg" alt="output"/>
			<img src="gist_res/test1 copy/2005_10_3_33.jpg" alt="output"/>
			<img src="gist_res/test1 copy/2005_3_37_6.jpg" alt="output"/>
			<img src="gist_res/test1 copy/2005_7_2_20.jpg" alt="output"/>
		</figure>

		<h3 style="padding-top:30px"> Test Result 3 </h3>

		<figure>
			<img src="gist_res/test3/input.jpg" alt="test3"/>
			<img src="gist_res/test3/mask.jpg" alt="mask"/>
		</figure>

		<figure class="result">
			<img src="gist_res/test3/after4.jpg" alt="output"/>
			<img src="gist_res/test3/after5.jpg" alt="output"/>
			<img src="gist_res/test3/after20.jpg" alt="output"/>
			<img src="gist_res/test3/after6.jpg" alt="output"/>
		</figure>
		<figure class="result">
			<img src="gist_res/test3/2007_6_2_3.jpg" alt="output"/>
			<img src="gist_res/test3/2007_7_6_37.jpg" alt="output"/>
			<img src="gist_res/test3/2013_9_13_59.jpg" alt="output"/>
			<img src="gist_res/test3/2008_5_4_23.jpg" alt="output"/>
		</figure>

		<h2 id="6" style="padding-top:30px"> REFERENCE </h2>

		<p> James Hays, Alexei A. Efros. Scene Completion Using Millions of Photographs. ACM Transactions on Graphics (SIGGRAPH 2007). August 2007, vol. 26, No. 3. </p>

		<p> Codes from <a href="http://qilezhi2.web.engr.illinois.edu/cs445/proj2/index.html"><b>Project 2</b></a> and <a href="http://qilezhi2.web.engr.illinois.edu/cs445/proj3/index.html"><b>Project 3</b></a>. </p>
		<p> <a href="https://github.com/tuttieee/lear-gist-python">GIST descriptor</a> by tuttieee from Github </p>


</body>

</html>
